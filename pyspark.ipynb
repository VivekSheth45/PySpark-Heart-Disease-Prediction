{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 261,
   "id": "980c0e59",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import VectorAssembler, StandardScaler, OneHotEncoder, StringIndexer\n",
    "from pyspark.ml.classification import LogisticRegression, DecisionTreeClassifier\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator, MulticlassClassificationEvaluator\n",
    "import pyspark.sql.functions as F\n",
    "import pyspark.sql.types as T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "id": "18dea06e-9d26-43e8-8c3f-ad9ce319e16f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File exists: True\n",
      "File size: 56674\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "file_path = \"/Users/vivek/Desktop/school/BIGDATA/heart.csv\"\n",
    "print(\"File exists:\", os.path.exists(file_path))  # Should return True\n",
    "print(\"File size:\", os.path.getsize(file_path))  # Should return file size in bytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "id": "dc4791af",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Heart Disease Analysis\") \\\n",
    "    .master(\"spark://10.100.165.4:7077\") \\\n",
    "    .config(\"spark.executor.memory\", \"2g\") \\\n",
    "    .config(\"spark.driver.memory\", \"1g\") \\\n",
    "    .config(\"spark.executor.cores\", \"2\") \\\n",
    "    .config(\"spark.cores.max\", \"6\") \\\n",
    "    .config(\"spark.submit.deployMode\", \"cluster\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "id": "8205a8e9-d62a-4199-9a94-32d5823fe2b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark session initialized: spark://10.100.165.4:7077\n"
     ]
    }
   ],
   "source": [
    "print(\"Spark session initialized:\", spark.conf.get(\"spark.master\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "id": "163c9606-09a3-4a34-aae8-49a1498bb8f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+---+------+----+---+-------+--------+----+-------+---+---+-----+------+\n",
      "|age|sex| cp|trtbps|chol|fbs|restecg|thalachh|exng|oldpeak|slp|caa|thall|output|\n",
      "+---+---+---+------+----+---+-------+--------+----+-------+---+---+-----+------+\n",
      "| 63|  1|  3|   145| 233|  1|      0|     150|   0|    2.3|  0|  0|    1|     1|\n",
      "| 37|  1|  2|   130| 250|  0|      1|     187|   0|    3.5|  0|  0|    2|     1|\n",
      "| 41|  0|  1|   130| 204|  0|      0|     172|   0|    1.4|  2|  0|    2|     1|\n",
      "| 56|  1|  1|   120| 236|  0|      1|     178|   0|    0.8|  2|  0|    2|     1|\n",
      "| 57|  0|  0|   120| 354|  0|      1|     163|   1|    0.6|  2|  0|    2|     1|\n",
      "| 57|  1|  0|   140| 192|  0|      1|     148|   0|    0.4|  1|  0|    1|     1|\n",
      "| 56|  0|  1|   140| 294|  0|      0|     153|   0|    1.3|  1|  0|    2|     1|\n",
      "| 44|  1|  1|   120| 263|  0|      1|     173|   0|    0.0|  2|  0|    3|     1|\n",
      "| 52|  1|  2|   172| 199|  1|      1|     162|   0|    0.5|  2|  0|    3|     1|\n",
      "| 57|  1|  2|   150| 168|  0|      1|     174|   0|    1.6|  2|  0|    2|     1|\n",
      "| 54|  1|  0|   140| 239|  0|      1|     160|   0|    1.2|  2|  0|    2|     1|\n",
      "| 48|  0|  2|   130| 275|  0|      1|     139|   0|    0.2|  2|  0|    2|     1|\n",
      "| 49|  1|  1|   130| 266|  0|      1|     171|   0|    0.6|  2|  0|    2|     1|\n",
      "| 64|  1|  3|   110| 211|  0|      0|     144|   1|    1.8|  1|  0|    2|     1|\n",
      "| 58|  0|  3|   150| 283|  1|      0|     162|   0|    1.0|  2|  0|    2|     1|\n",
      "| 50|  0|  2|   120| 219|  0|      1|     158|   0|    1.6|  1|  0|    2|     1|\n",
      "| 58|  0|  2|   120| 340|  0|      1|     172|   0|    0.0|  2|  0|    2|     1|\n",
      "| 66|  0|  3|   150| 226|  0|      1|     114|   0|    2.6|  0|  0|    2|     1|\n",
      "| 43|  1|  0|   150| 247|  0|      1|     171|   0|    1.5|  2|  0|    2|     1|\n",
      "| 69|  0|  3|   140| 239|  0|      1|     151|   0|    1.8|  2|  2|    2|     1|\n",
      "+---+---+---+------+----+---+-------+--------+----+-------+---+---+-----+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.csv(\"/Users/vivek/Desktop/school/BIGDATA/heart.csv\", header=True, inferSchema=True)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "id": "973fce71-4e02-4031-abad-cbea50b7042e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['age,sex,cp,trtbps,chol,fbs,restecg,thalachh,exng,oldpeak,slp,caa,thall,output', '63,1,3,145,233,1,0,150,0,2.3,0,0,1,1', '37,1,2,130,250,0,1,187,0,3.5,0,0,2,1', '41,0,1,130,204,0,0,172,0,1.4,2,0,2,1', '56,1,1,120,236,0,1,178,0,0.8,2,0,2,1']\n"
     ]
    }
   ],
   "source": [
    "rdd = spark.sparkContext.textFile(\"/Users/vivek/Desktop/school/BIGDATA/heart.csv\")\n",
    "print(rdd.take(5))  # This will print the first 5 lines if the file is accessible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "id": "50ffcdca",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.csv(\"/Users/vivek/Desktop/school/BIGDATA/heart.csv\", header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "id": "bb2190a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename columns (equivalent to your original column renaming)\n",
    "new_cols = [\"age\", \"sex\", \"cp\", \"trtbps\", \"chol\", \"fbs\", \"rest_ecg\", \"thalach\", \"exang\", \"oldpeak\", \"slope\", \"ca\", \"thal\", \"target\"]\n",
    "for old, new in zip(df.columns, new_cols):\n",
    "    df = df.withColumnRenamed(old, new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "id": "6f660882",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handle missing values\n",
    "df = df.na.drop(subset=[\"age\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "id": "174420c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature preprocessing pipeline\n",
    "# Identify numeric and categorical columns\n",
    "numeric_cols = [\"age\", \"trtbps\", \"thalach\", \"oldpeak\"]\n",
    "categorical_cols = [\"sex\", \"cp\", \"exang\", \"slope\", \"ca\", \"thal\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "id": "e2852231",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create preprocessing stages\n",
    "stages = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "id": "20476d6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-hot encode categorical variables\n",
    "for cat_col in categorical_cols:\n",
    "    string_indexer = StringIndexer(inputCol=cat_col, outputCol=f\"{cat_col}_index\")\n",
    "    onehot_encoder = OneHotEncoder(inputCols=[f\"{cat_col}_index\"], \n",
    "                                   outputCols=[f\"{cat_col}_encoded\"])\n",
    "    stages.extend([string_indexer, onehot_encoder])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "id": "72726b9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assemble numeric features\n",
    "assembler_inputs = numeric_cols + [f\"{col}_encoded\" for col in categorical_cols]\n",
    "vector_assembler = VectorAssembler(inputCols=assembler_inputs, outputCol=\"features\")\n",
    "stages.append(vector_assembler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "id": "bb19359a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale features\n",
    "scaler = StandardScaler(inputCol=\"features\", outputCol=\"scaled_features\")\n",
    "stages.append(scaler)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce9d77d9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "id": "f7dbd15f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a logistic regression model\n",
    "lr = LogisticRegression(featuresCol=\"scaled_features\", labelCol=\"target\")\n",
    "stages.append(lr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "id": "ecde34fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and fit the pipeline\n",
    "pipeline = Pipeline(stages=stages)\n",
    "model = pipeline.fit(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "id": "669df4c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data\n",
    "train_df, test_df = df.randomSplit([0.8, 0.2], seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "id": "d31aa2fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions\n",
    "predictions = model.transform(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "id": "87b9fbb6-9541-4f5f-8adc-4704dc76f1a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC: 0.931465517241378\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/11/28 13:10:38 WARN SparkConf: The configuration key 'spark.executor.port' has been deprecated as of Spark 2.0.0 and may be removed in the future. Not used anymore\n",
      "24/11/28 13:10:38 WARN SparkConf: The configuration key 'spark.executor.port' has been deprecated as of Spark 2.0.0 and may be removed in the future. Not used anymore\n",
      "24/11/28 13:10:38 WARN SparkConf: The configuration key 'spark.executor.port' has been deprecated as of Spark 2.0.0 and may be removed in the future. Not used anymore\n",
      "24/11/28 13:10:38 WARN SparkConf: The configuration key 'spark.executor.port' has been deprecated as of Spark 2.0.0 and may be removed in the future. Not used anymore\n",
      "24/11/28 13:10:38 WARN SparkConf: The configuration key 'spark.executor.port' has been deprecated as of Spark 2.0.0 and may be removed in the future. Not used anymore\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model\n",
    "evaluator = BinaryClassificationEvaluator(labelCol=\"target\")\n",
    "auc = evaluator.evaluate(predictions)\n",
    "print(f\"AUC: {auc}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "id": "5fe7e46c-ee74-4bf5-a909-b320ba7c3c66",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8832684824902723\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/11/28 13:10:38 WARN SparkConf: The configuration key 'spark.executor.port' has been deprecated as of Spark 2.0.0 and may be removed in the future. Not used anymore\n",
      "24/11/28 13:10:39 WARN SparkConf: The configuration key 'spark.executor.port' has been deprecated as of Spark 2.0.0 and may be removed in the future. Not used anymore\n"
     ]
    }
   ],
   "source": [
    "multi_evaluator = MulticlassClassificationEvaluator(labelCol=\"target\", metricName=\"accuracy\")\n",
    "accuracy = multi_evaluator.evaluate(predictions)\n",
    "print(f\"Accuracy: {accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcb851fa-11ef-454d-be98-34ac0d747067",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
